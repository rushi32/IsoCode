# IsoCode Agent Server Configuration
# Copy this file to .env and adjust values as needed.

# ---- LLM Provider ----
# Auto-detected from LLM_API_BASE, or set explicitly:
# Options: ollama, lmstudio, openai
# LLM_PROVIDER=ollama

# ---- Ollama (default) ----
LLM_API_BASE=http://localhost:11434/v1
LLM_MODEL_ID=qwen3-coder
LLM_API_KEY=ollama

# ---- LM Studio ----
# LLM_API_BASE=http://localhost:1234/v1
# LLM_MODEL_ID=local-model
# LLM_API_KEY=lm-studio

# ---- OpenAI ----
# LLM_API_BASE=https://api.openai.com/v1
# LLM_MODEL_ID=gpt-4o-mini
# LLM_API_KEY=sk-your-api-key-here

# ---- Server ----
PORT=3000

# ---- Permissions ----
# Options: ask, always, never
# PERMISSIONS_SHELL=ask
# PERMISSIONS_WRITE=ask
# PERMISSIONS_EDIT=ask

# ---- Context ----
# CONTEXT_WINDOW_SIZE=16384
# MAX_HISTORY_MESSAGES=30
# TEMPERATURE=0.7

# ---- MCP Servers (JSON array, set in extension settings UI) ----
# Example: [{"name":"filesystem","command":"npx","args":["-y","@modelcontextprotocol/server-filesystem","D:/projects"]}]
# MCP_SERVERS=[]
