# IsoCode Agent Server Configuration
# Copy this file to .env and adjust values as needed.

# ---- LLM Provider ----
# Auto-detected from LLM_API_BASE, or set explicitly:
# Options: ollama, lmstudio, openai
# LLM_PROVIDER=ollama

# ---- Ollama (default) ----
LLM_API_BASE=http://localhost:11434/v1
LLM_MODEL_ID=qwen3-coder
LLM_API_KEY=ollama

# ---- LM Studio ----
# LLM_API_BASE=http://localhost:1234/v1
# LLM_MODEL_ID=local-model
# LLM_API_KEY=lm-studio

# ---- OpenAI ----
# LLM_API_BASE=https://api.openai.com/v1
# LLM_MODEL_ID=gpt-4o-mini
# LLM_API_KEY=sk-your-api-key-here

# ---- Server ----
PORT=3000

# ---- Permissions ----
# Options: ask, always, never
# PERMISSIONS_SHELL=ask
# PERMISSIONS_WRITE=ask
# PERMISSIONS_EDIT=ask

# ---- Context ----
# CONTEXT_WINDOW_SIZE=16384
# MAX_HISTORY_MESSAGES=30
# TEMPERATURE=0.7

# ---- Agent+ Swarm (optional) ----
# Max parallel workers when the model delegates subtasks. Kept low (default 2) to avoid GPU/memory bottleneck.
# If swarm fails (e.g. OOM), the run automatically continues in single-agent mode.
# SWARM_MAX_WORKERS=2
# Fallback vision model for swarm if no vision-capable model is found in the provider. The server otherwise
# auto-picks the best model per subtask from available models (vision vs coder vs general).
# SWARM_VISION_MODEL=llava

# ---- System prompt (optional) ----
# Path to a custom system prompt file; if unset, built-in prompt is used.
# SYSTEM_PROMPT_PATH=./custom-system-prompt.txt

# ---- MCP Servers (JSON array, set in extension settings UI) ----
# Example: [{"name":"filesystem","command":"npx","args":["-y","@modelcontextprotocol/server-filesystem","D:/projects"]}]
# MCP_SERVERS=[]
